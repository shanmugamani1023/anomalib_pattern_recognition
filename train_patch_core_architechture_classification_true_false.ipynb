{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Lambda(lambda img: img.convert('RGB')),  # Convert to RGB if needed\n",
    "#     transforms.Resize((512, 512)),  # Resize images to a consistent size\n",
    "#     transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from anomalib import TaskType\n",
    "from anomalib.data import MVTec\n",
    "from anomalib.data.utils import read_image\n",
    "from anomalib.deploy import ExportType, OpenVINOInferencer\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Shravtek\\\\neutech_airfillter\\\\anamolib'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Shravtek\\neutech_airfillter\\anamolib\\anomalib\n"
     ]
    }
   ],
   "source": [
    "cd anomalib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Shravtek\\\\neutech_airfillter\\\\anamolib\\\\anomalib'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datamodule\n",
    "from anomalib.data import Folder\n",
    "\n",
    "# Create the datamodule\n",
    "datamodule = Folder(\n",
    "    name=\"hazelnut_toy\",\n",
    "    root=\"datasets/hazelnut_toy\",\n",
    "    normal_dir=\"good\",\n",
    "    abnormal_dir=\"crack\",\n",
    "    task=\"classification\",\n",
    "    image_size=(512,512)\n",
    "  )\n",
    "\n",
    "# Setup the datamodule\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image'])\n",
      "dict_keys(['image_path', 'label', 'image'])\n",
      "dict_keys(['image_path', 'label', 'image'])\n"
     ]
    }
   ],
   "source": [
    "i, train_data = next(enumerate(datamodule.train_dataloader()))\n",
    "print(train_data.keys())\n",
    "# dict_keys(['image_path', 'label', 'image'])\n",
    "\n",
    "i, val_data = next(enumerate(datamodule.val_dataloader()))\n",
    "print(val_data.keys())\n",
    "# dict_keys(['image_path', 'label', 'image'])\n",
    "\n",
    "i, test_data = next(enumerate(datamodule.test_dataloader()))\n",
    "print(test_data.keys())\n",
    "# dict_keys(['image_path', 'label', 'image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model and engine\n",
    "from anomalib.models import Patchcore\n",
    "from anomalib.engine import Engine\n",
    "\n",
    "# Create the model and engine\n",
    "model = Patchcore()\n",
    "engine = Engine(task=\"classification\",max_epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001cdb75284246d9a11bb56a5100d7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:129: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bda86ff75443b68d1742922dd319f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e6f43583cb4569bb2e97e44387ca9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d0a1d040124d17b7e42bc2de2d0f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.800000011920929     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.800000011920929    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train a Patchcore model on the given datamodule\n",
    "engine.train(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "Restoring states from the checkpoint path at D:\\Shravtek\\neutech_airfillter\\anamolib\\anomalib\\results\\Patchcore\\hazelnut_toy\\v2\\weights\\lightning\\model.ckpt\n",
      "Loaded model weights from the checkpoint at D:\\Shravtek\\neutech_airfillter\\anamolib\\anomalib\\results\\Patchcore\\hazelnut_toy\\v2\\weights\\lightning\\model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525b67cd5f4742ada882094aad00611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = engine.test(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=engine.trainer.checkpoint_callback.best_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_AUROC': 1.0, 'image_F1Score': 1.0}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Inference\n",
    "\n",
    "Now that we trained and tested a model, we could check a single inference result using OpenVINO inferencer object. This will demonstrate how a trained model could be used for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if image.numel() == 0:\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:61: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if crop_height > image_height or crop_width > image_width:\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:41: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  crop_top = torch.tensor((image_height - crop_height) / 2.0).round().int().item()\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  crop_top = torch.tensor((image_height - crop_height) / 2.0).round().int().item()\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:41: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  crop_top = torch.tensor((image_height - crop_height) / 2.0).round().int().item()\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:42: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  crop_left = torch.tensor((image_width - crop_width) / 2.0).round().int().item()\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  crop_left = torch.tensor((image_width - crop_width) / 2.0).round().int().item()\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\data\\transforms\\center_crop.py:42: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  crop_left = torch.tensor((image_width - crop_width) / 2.0).round().int().item()\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\anomalib\\models\\image\\patchcore\\torch_model.py:225: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  n_neighbors=min(self.num_neighbors, memory_bank_effective_size),\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5857: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\torch\\onnx\\utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\shan\\.conda\\envs\\anamo\\lib\\site-packages\\torch\\onnx\\utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/Shravtek/neutech_airfillter/anamolib/anomalib/results/Patchcore/hazelnut_toy/latest/weights/openvino/model.xml')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.export(\n",
    "    model=model,\n",
    "    export_type=ExportType.OPENVINO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16d987a0dc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"D:/Shravtek/neutech_airfillter/anamolib/anomalib/datasets/hazelnut_toy/crack/01.jpg\"\n",
    "image = read_image(path=\"D:/Shravtek/neutech_airfillter/anamolib/anomalib/datasets/hazelnut_toy/crack/01.jpg\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Shravtek\\neutech_airfillter\\anamolib\\anomalib\\results\\Patchcore\\hazelnut_toy\\latest\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(engine.trainer.default_root_dir)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "openvino_model_path = output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "metadata = output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "print(openvino_model_path.exists(), metadata.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,  # Path to the OpenVINO IR model.\n",
    "    metadata=metadata,  # Path to the metadata file.\n",
    "    device=\"CPU\",  # We would like to run it on an Intel CPU.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inferencer.predict(image=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55394659682092 LabelName.ABNORMAL\n"
     ]
    }
   ],
   "source": [
    "print(predictions.pred_score, predictions.pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageResult(image=[[[228 214  79]\n",
      "  [231 217  82]\n",
      "  [231 217  82]\n",
      "  ...\n",
      "  [226 213  82]\n",
      "  [225 213  79]\n",
      "  [229 217  83]]\n",
      "\n",
      " [[228 214  79]\n",
      "  [228 214  79]\n",
      "  [229 215  80]\n",
      "  ...\n",
      "  [226 214  80]\n",
      "  [227 215  81]\n",
      "  [228 216  80]]\n",
      "\n",
      " [[230 217  79]\n",
      "  [230 217  79]\n",
      "  [229 216  78]\n",
      "  ...\n",
      "  [225 213  79]\n",
      "  [226 214  78]\n",
      "  [222 210  74]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[233 224  83]\n",
      "  [232 223  82]\n",
      "  [232 223  82]\n",
      "  ...\n",
      "  [223 211  73]\n",
      "  [223 211  73]\n",
      "  [222 210  72]]\n",
      "\n",
      " [[233 224  83]\n",
      "  [229 220  79]\n",
      "  [235 226  85]\n",
      "  ...\n",
      "  [225 214  73]\n",
      "  [223 212  71]\n",
      "  [224 213  72]]\n",
      "\n",
      " [[230 221  80]\n",
      "  [230 221  80]\n",
      "  [231 222  81]\n",
      "  ...\n",
      "  [219 208  67]\n",
      "  [221 210  69]\n",
      "  [226 215  74]]], pred_score=0.55394659682092, pred_label=1, anomaly_map=[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], gt_mask=None, gt_boxes=None, pred_boxes=None, box_labels=None, pred_mask=[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], heat_map=[[[137 128  99]\n",
      "  [139 130 100]\n",
      "  [139 130 100]\n",
      "  ...\n",
      "  [136 128 100]\n",
      "  [135 128  99]\n",
      "  [137 130 101]]\n",
      "\n",
      " [[137 128  99]\n",
      "  [137 128  99]\n",
      "  [137 129  99]\n",
      "  ...\n",
      "  [136 128  99]\n",
      "  [136 129 100]\n",
      "  [137 130  99]]\n",
      "\n",
      " [[138 130  99]\n",
      "  [138 130  99]\n",
      "  [137 130  98]\n",
      "  ...\n",
      "  [135 128  99]\n",
      "  [136 128  98]\n",
      "  [133 126  96]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[140 134 101]\n",
      "  [139 134 100]\n",
      "  [139 134 100]\n",
      "  ...\n",
      "  [134 127  95]\n",
      "  [134 127  95]\n",
      "  [133 126  94]]\n",
      "\n",
      " [[140 134 101]\n",
      "  [137 132  99]\n",
      "  [141 136 102]\n",
      "  ...\n",
      "  [135 128  95]\n",
      "  [134 127  94]\n",
      "  [134 128  94]]\n",
      "\n",
      " [[138 133  99]\n",
      "  [138 133  99]\n",
      "  [139 133 100]\n",
      "  ...\n",
      "  [131 125  91]\n",
      "  [133 126  93]\n",
      "  [136 129  96]]], segmentations=[[[227 214  79]\n",
      "  [231 217  81]\n",
      "  [231 217  81]\n",
      "  ...\n",
      "  [226 213  81]\n",
      "  [225 213  79]\n",
      "  [229 217  83]]\n",
      "\n",
      " [[227 214  79]\n",
      "  [227 214  79]\n",
      "  [229 215  80]\n",
      "  ...\n",
      "  [226 214  80]\n",
      "  [227 215  81]\n",
      "  [227 216  80]]\n",
      "\n",
      " [[230 217  79]\n",
      "  [230 217  79]\n",
      "  [229 216  78]\n",
      "  ...\n",
      "  [225 213  79]\n",
      "  [226 214  78]\n",
      "  [222 210  73]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[233 224  83]\n",
      "  [232 223  81]\n",
      "  [232 223  81]\n",
      "  ...\n",
      "  [223 211  73]\n",
      "  [223 211  73]\n",
      "  [222 210  72]]\n",
      "\n",
      " [[233 224  83]\n",
      "  [229 220  79]\n",
      "  [235 226  85]\n",
      "  ...\n",
      "  [225 214  73]\n",
      "  [223 211  71]\n",
      "  [224 213  72]]\n",
      "\n",
      " [[230 221  80]\n",
      "  [230 221  80]\n",
      "  [231 222  81]\n",
      "  ...\n",
      "  [219 208  67]\n",
      "  [221 210  69]\n",
      "  [226 215  73]]])\n"
     ]
    }
   ],
   "source": [
    "# Visualize the original image\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anamo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
